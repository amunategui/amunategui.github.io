---
---
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Machine Learning, R Programming, Statistics, Artificial Intelligence">
    <meta name="author" content="Manuel Amunategui">
    <link rel="icon" href="../favicon.ico">

    <title>Data Exploration & Machine Learning, Hands-on</title>

    {% include externals.html %}
  
</head>

  

<body>

<main role="main">

{% include header.html %}
   
{% include signup.html %}


<div class="container">
  <div class="blog-header">
    <h1 class="blog-title">Actionable Insights: Getting Variable Importance at the Prediction Level in R</h1>
    <p class="lead blog-description">Practical walkthroughs on machine learning, data exploration and finding insight.</p>
  </div>
  


    <p><br /></p>
<p style="text-align:center">
<img src="img/ROBOT.png" alt="Actionable Insights" style="padding:1px; border:1px solid #021a40; width: 30%; height: 30%" /></p>

 
<p><strong>Resources</strong></p>
<ul>
<li type="square"><a href="https://www.youtube.com/watch?v=gs74y1R-uEw&amp;index=1&amp;list=UUq4pm1i_VZqxKVVOz5qRBIA" target="_blank">YouTube Companion Video</a></li>
</ul>
<p><br /><br /></p>

<p>When we talk of variable importance we most often think of variables at the aggregate level of a supervised task. This is useful to understand a model at a high level but falls short in terms of actionable insight. Report readers want to know why a particular observation is given a particular probability - knowing full well that each prediction is a different situation.</p>

{% include follow-me.html %}

<p>An easy way to extract the top variables for each observation is to simply cycle through each feature, average it to the population mean, and compare it to the original prediction. If the probability changes for that observation, then that feature has a strong effect for the observation. As probabilities share the same scale, we can measure the degree of change and sort each feature directionally.</p>

<p>Let’s see this through an example.</p>

<p>We’ll use the <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes" target="_blank">Pima Indians Diabetes Database from the UCI Machine Learning Repository</a>. The data represents 768 patient observations and a series of medical measures to predict signs of diabetes.</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="w">
</span><span class="n">pima_db_names</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'Number_times_pregnant'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Plasma_glucose'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Diastolic_blood_pressure'</span><span class="p">,</span><span class="w"> 
                   </span><span class="s1">'Triceps_skin_fold_thickness'</span><span class="p">,</span><span class="s1">'2_Hour_insulin'</span><span class="p">,</span><span class="w"> </span><span class="s1">'BMI'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Diabetes_pedigree'</span><span class="p">,</span><span class="w">
                   </span><span class="s1">'Age'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Class'</span><span class="p">)</span><span class="w">

</span><span class="n">pima_db</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s1">'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'</span><span class="p">,</span><span class="w">
                    </span><span class="n">col.names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pima_db_names</span><span class="p">)</span><span class="w">


</span><span class="c1"># removing obscure features
</span><span class="n">pima_db</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pima_db</span><span class="p">[,</span><span class="nf">c</span><span class="p">(</span><span class="s2">"Number_times_pregnant"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Plasma_glucose"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Diastolic_blood_pressure"</span><span class="p">,</span><span class="s2">"BMI"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Age"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Class"</span><span class="p">)]</span><span class="w">

</span></code></pre>
</div>
<p><br /><br /> 
Let’s take a peek at the data:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="w">
</span><span class="nf">dim</span><span class="p">(</span><span class="n">pima_db</span><span class="p">)</span><span class="w">

</span><span class="c1">## [1] 767   6
</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">pima_db</span><span class="p">)</span><span class="w">

</span><span class="c1">##   Number_times_pregnant Plasma_glucose Diastolic_blood_pressure  BMI Age
## 1                     1             85                       66 26.6  31
## 2                     8            183                       64 23.3  32
## 3                     1             89                       66 28.1  21
## 4                     0            137                       40 43.1  33
## 5                     5            116                       74 25.6  30
## 6                     3             78                       50 31.0  26
##   Class
## 1     0
## 2     1
## 3     0
## 4     1
## 5     0
## 6     1
</span><span class="w">
</span></code></pre>
</div>
<p><br /><br /> 
The data is almost model-ready out of the box and we just need to split it into train/test sets:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">1234</span><span class="p">)</span><span class="w">
</span><span class="n">random_splits</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">runif</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">pima_db</span><span class="p">))</span><span class="w">
</span><span class="n">train_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pima_db</span><span class="p">[</span><span class="n">random_splits</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">.5</span><span class="p">,]</span><span class="w">
</span><span class="nf">dim</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span><span class="w">

</span><span class="c1">## [1] 367   6
</span><span class="w">
</span><span class="n">test_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pima_db</span><span class="p">[</span><span class="n">random_splits</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="m">.5</span><span class="p">,]</span><span class="w">
</span><span class="nf">dim</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span><span class="w">

</span><span class="c1">## [1] 400   6
</span><span class="w">
</span><span class="n">outcome_name</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s1">'Class'</span><span class="w">
</span></code></pre>
</div>
<p><br /><br />
To simplify things and maybe make this more useful for your advanced-analytics pipeline, we’ll build our prediction insight program into a function. The modeling engine will use
<a href="https://cran.r-project.org/web/packages/xgboost/index.html" target="_blank">xgboost: Extreme Gradient Boosting</a> because it is easy to use and fast. You will need to install <code class="highlighter-rouge">xgboost</code> and <code class="highlighter-rouge">dplyr</code> if you don’t already have them (both available on cran).</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">observation_level_variable_importance</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span><span class="w"> </span><span class="n">live_data</span><span class="p">,</span><span class="w"> </span><span class="n">outcome_name</span><span class="p">,</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.2</span><span class="p">,</span><span class="w"> 
                                                  </span><span class="n">max_depth</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">max_rounds</span><span class="o">=</span><span class="m">3000</span><span class="p">,</span><span class="w"> </span><span class="n">number_of_factors</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
          
     </span><span class="c1"># install.packages('dplyr')
</span><span class="w">     </span><span class="n">require</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
     </span><span class="c1"># install.packages('xgboost')
</span><span class="w">     </span><span class="n">require</span><span class="p">(</span><span class="n">xgboost</span><span class="p">)</span><span class="w">
     
     </span><span class="n">set.seed</span><span class="p">(</span><span class="m">1234</span><span class="p">)</span><span class="w">
     </span><span class="n">split</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span><span class="w"> </span><span class="nf">floor</span><span class="p">(</span><span class="m">0.9</span><span class="o">*</span><span class="n">nrow</span><span class="p">(</span><span class="n">train_data</span><span class="p">)))</span><span class="w">
     </span><span class="n">train_data_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">train_data</span><span class="p">[</span><span class="n">split</span><span class="p">,]</span><span class="w">
     </span><span class="n">val_data_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">train_data</span><span class="p">[</span><span class="o">-</span><span class="n">split</span><span class="p">,]</span><span class="w">
     
     </span><span class="n">feature_names</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">setdiff</span><span class="p">(</span><span class="nf">names</span><span class="p">(</span><span class="n">train_data_tmp</span><span class="p">),</span><span class="w"> </span><span class="n">outcome_name</span><span class="p">)</span><span class="w">
     </span><span class="n">dtrain</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.DMatrix</span><span class="p">(</span><span class="n">data.matrix</span><span class="p">(</span><span class="n">train_data_tmp</span><span class="p">[,</span><span class="n">feature_names</span><span class="p">]),</span><span class="w"> 
                           </span><span class="n">label</span><span class="o">=</span><span class="n">train_data_tmp</span><span class="p">[,</span><span class="n">outcome_name</span><span class="p">],</span><span class="w"> </span><span class="n">missing</span><span class="o">=</span><span class="kc">NaN</span><span class="p">)</span><span class="w">
     </span><span class="n">dval</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.DMatrix</span><span class="p">(</span><span class="n">data.matrix</span><span class="p">(</span><span class="n">val_data_tmp</span><span class="p">[,</span><span class="n">feature_names</span><span class="p">]),</span><span class="w"> 
                         </span><span class="n">label</span><span class="o">=</span><span class="n">val_data_tmp</span><span class="p">[,</span><span class="n">outcome_name</span><span class="p">],</span><span class="w"> </span><span class="n">missing</span><span class="o">=</span><span class="kc">NaN</span><span class="p">)</span><span class="w">
     </span><span class="n">watchlist</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">eval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dval</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dtrain</span><span class="p">)</span><span class="w">
     </span><span class="n">param</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">  </span><span class="n">objective</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"binary:logistic"</span><span class="p">,</span><span class="w">
                     </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eta</span><span class="p">,</span><span class="w">
                     </span><span class="n">max_depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_depth</span><span class="p">,</span><span class="w">
                     </span><span class="n">subsample</span><span class="o">=</span><span class="w"> </span><span class="m">0.9</span><span class="p">,</span><span class="w">
                     </span><span class="n">colsample_bytree</span><span class="o">=</span><span class="w"> </span><span class="m">0.9</span><span class="w">
     </span><span class="p">)</span><span class="w">
     
     </span><span class="n">xgb_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.train</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="p">,</span><span class="w">
                              </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dtrain</span><span class="p">,</span><span class="w">
                              </span><span class="n">eval_metric</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"auc"</span><span class="p">,</span><span class="w">
                              </span><span class="n">nrounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_rounds</span><span class="p">,</span><span class="w">
                              </span><span class="n">missing</span><span class="o">=</span><span class="kc">NaN</span><span class="p">,</span><span class="w">
                              </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w">
                              </span><span class="n">print.every.n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
                              </span><span class="n">early.stop.round</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w">
                              </span><span class="n">watchlist</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">watchlist</span><span class="p">,</span><span class="w">
                              </span><span class="n">maximize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
     
     </span><span class="n">original_predictions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">xgb_model</span><span class="p">,</span><span class="w"> 
                                     </span><span class="n">data.matrix</span><span class="p">(</span><span class="n">live_data</span><span class="p">[,</span><span class="n">feature_names</span><span class="p">]),</span><span class="w"> 
                                     </span><span class="n">outputmargin</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">missing</span><span class="o">=</span><span class="kc">NaN</span><span class="p">)</span><span class="w">
      
     </span><span class="c1"># strongest factors
</span><span class="w">     </span><span class="n">new_preds</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
     </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">feature</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">feature_names</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="n">print</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span><span class="w">
          </span><span class="n">live_data_trsf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">live_data</span><span class="w">
          </span><span class="c1"># neutralize feature to population mean
</span><span class="w">          </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="nf">is.na</span><span class="p">(</span><span class="n">train_data</span><span class="p">[,</span><span class="n">feature</span><span class="p">]))</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
               </span><span class="n">live_data_trsf</span><span class="p">[,</span><span class="n">feature</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kc">NA</span><span class="w">
          </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w">
               </span><span class="n">live_data_trsf</span><span class="p">[,</span><span class="n">feature</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">train_data</span><span class="p">[,</span><span class="n">feature</span><span class="p">],</span><span class="w"> </span><span class="n">na.rm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
          </span><span class="p">}</span><span class="w">
          </span><span class="n">predictions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">object</span><span class="o">=</span><span class="n">xgb_model</span><span class="p">,</span><span class="w"> </span><span class="n">data.matrix</span><span class="p">(</span><span class="n">live_data_trsf</span><span class="p">[,</span><span class="n">feature_names</span><span class="p">]),</span><span class="w">
                                 </span><span class="n">outputmargin</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">missing</span><span class="o">=</span><span class="kc">NaN</span><span class="p">)</span><span class="w">
          </span><span class="n">new_preds</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="n">new_preds</span><span class="p">,</span><span class="w"> </span><span class="n">original_predictions</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">predictions</span><span class="p">)</span><span class="w">
     </span><span class="p">}</span><span class="w">
     
     </span><span class="n">positive_features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
     </span><span class="n">negative_features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
     
     </span><span class="n">feature_effect_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">new_preds</span><span class="p">)</span><span class="w">
     </span><span class="nf">names</span><span class="p">(</span><span class="n">feature_effect_df</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)</span><span class="w">
     
     </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">pred_id</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">feature_effect_df</span><span class="p">)))</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="n">vector_vals</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">feature_effect_df</span><span class="p">[</span><span class="n">pred_id</span><span class="p">,]</span><span class="w">
          </span><span class="n">vector_vals</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector_vals</span><span class="p">[,</span><span class="o">!</span><span class="nf">is.na</span><span class="p">(</span><span class="n">vector_vals</span><span class="p">)]</span><span class="w">
          </span><span class="n">positive_features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rbind</span><span class="p">(</span><span class="n">positive_features</span><span class="p">,</span><span class="w"> 
                                     </span><span class="nf">c</span><span class="p">(</span><span class="n">colnames</span><span class="p">(</span><span class="n">vector_vals</span><span class="p">)[</span><span class="n">order</span><span class="p">(</span><span class="n">vector_vals</span><span class="p">,</span><span class="w"> 
                                                                   </span><span class="n">decreasing</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)][</span><span class="m">1</span><span class="o">:</span><span class="n">number_of_factors</span><span class="p">]))</span><span class="w">
          </span><span class="n">negative_features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rbind</span><span class="p">(</span><span class="n">negative_features</span><span class="p">,</span><span class="w"> 
                                     </span><span class="nf">c</span><span class="p">(</span><span class="n">colnames</span><span class="p">(</span><span class="n">vector_vals</span><span class="p">)[</span><span class="n">order</span><span class="p">(</span><span class="n">vector_vals</span><span class="p">,</span><span class="w"> 
                                                                   </span><span class="n">decreasing</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)][</span><span class="m">1</span><span class="o">:</span><span class="n">number_of_factors</span><span class="p">]))</span><span class="w">
     </span><span class="p">}</span><span class="w">
     
     </span><span class="n">positive_features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">positive_features</span><span class="p">)</span><span class="w">
     </span><span class="nf">names</span><span class="p">(</span><span class="n">positive_features</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="s1">'Pos_'</span><span class="p">,</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">positive_features</span><span class="p">))</span><span class="w">
     </span><span class="n">negative_features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">negative_features</span><span class="p">)</span><span class="w">
     </span><span class="nf">names</span><span class="p">(</span><span class="n">negative_features</span><span class="p">)</span><span class="w"> </span><span class="o">&ltlt;-</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="s1">'Neg_'</span><span class="p">,</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">negative_features</span><span class="p">))</span><span class="w">
     
     </span><span class="nf">return</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="n">original_predictions</span><span class="p">,</span><span class="w"> </span><span class="n">positive_features</span><span class="p">,</span><span class="w"> </span><span class="n">negative_features</span><span class="p">))</span><span class="w">
      
</span><span class="p">}</span><span class="w"> 
</span></code></pre>
</div>
<p><br /><br /> 
The first half of the function is straight-forward <code class="highlighter-rouge">xgboost</code> classification (see <a href="https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html" target="_blank">XGBoost R Tutorial</a>) and we get a vector of predictions for our test/live data. It is in the second half that things get more interesting - after the model has trained on the training data split and predicted on the testing split, we are left with the prediction vector - dubbed original predictions. It is a long series of probabilities, one for each observation. The model used here isn’t important and the above should work with most models.</p>

<p>We then run through each feature a second time we reset the feature to the population mean. We feed that the new data set with its feature neutralized into the prediction function and compare the original prediction vector against this new one. Any time the prediction for an observation changes, we conclude it is important for that observation. We also record whether the original feature has a positive or negative influence on that prediction.</p>

<div class="highlighter-rouge"><pre class="highlight"><code># get variable importance 

preds &lt;- observation_level_variable_importance(train_data = train_df, 
                         live_data = test_df, 
                         outcome_name = outcome_name)

## Loading required package: dplyr

## 
## Attaching package: 'dplyr'

## The following objects are masked from 'package:stats':
## 
##     filter, lag

## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union

## Loading required package: xgboost

## 
## Attaching package: 'xgboost'

## The following object is masked from 'package:dplyr':
## 
##     slice

## Warning in xgb.train(params = param, data = dtrain, eval_metric = "auc", :
## Only the first data set in watchlist is used for early stopping process.

## [0]  eval-auc:0.800000   train-auc:0.841169
## [10] eval-auc:0.814815   train-auc:0.926103
## [20] eval-auc:0.818519   train-auc:0.948686
## [30] eval-auc:0.822222   train-auc:0.956050
## [40] eval-auc:0.803704   train-auc:0.965202
## Stopping. Best iteration: 25[1] "Number_times_pregnant"
## [1] "Plasma_glucose"
## [1] "Diastolic_blood_pressure"
## [1] "BMI"
## [1] "Age"

</code></pre>
</div>
<p><br /><br />
Let’s take a look at the most extreme probabilities - <code class="highlighter-rouge">Diastolic_blood_pressure</code> is a strong influence for a low probability score, while <code class="highlighter-rouge">Plasma_glucose</code> is a strong influence for a high probability score:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="w">
</span><span class="n">preds</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">preds</span><span class="p">[</span><span class="n">order</span><span class="p">(</span><span class="n">preds</span><span class="o">$</span><span class="n">original_predictions</span><span class="p">),]</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="w">

</span><span class="c1">##     original_predictions                   Pos_X1                   Pos_X2
## 42           0.003732002    Number_times_pregnant Diastolic_blood_pressure
## 346          0.003956458 Diastolic_blood_pressure    Number_times_pregnant
## 359          0.004147866 Diastolic_blood_pressure    Number_times_pregnant
## 274          0.004275108    Number_times_pregnant Diastolic_blood_pressure
## 13           0.004463046 Diastolic_blood_pressure    Number_times_pregnant
## 72           0.005090717    Number_times_pregnant Diastolic_blood_pressure
##     Neg_X1         Neg_X2
## 42     BMI            Age
## 346    BMI            Age
## 359    BMI            Age
## 274    BMI Plasma_glucose
## 13     BMI            Age
## 72     BMI            Age
</span><span class="w">
</span><span class="n">tail</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="w">

</span><span class="c1">##     original_predictions         Pos_X1                Pos_X2
## 129            0.9508100 Plasma_glucose Number_times_pregnant
## 248            0.9511197 Plasma_glucose                   BMI
## 203            0.9590986 Plasma_glucose Number_times_pregnant
## 71             0.9644873 Plasma_glucose                   Age
## 51             0.9722556 Plasma_glucose                   BMI
## 224            0.9739259 Plasma_glucose Number_times_pregnant
##                       Neg_X1                   Neg_X2
## 129                      BMI Diastolic_blood_pressure
## 248                      Age Diastolic_blood_pressure
## 203                      Age Diastolic_blood_pressure
## 71  Diastolic_blood_pressure                      BMI
## 51                       Age Diastolic_blood_pressure
## 224                      Age Diastolic_blood_pressure
</span></code></pre>
</div>
<p><br /><br />
To confirm this, let’s use a Generalized Linear Model (<code class="highlighter-rouge">glm</code>) from <a href="http://caret.r-forge.r-project.org/" target="_blank">caret</a> to access directional variable importance:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="c1"># install.packages('caret')
</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">caret</span><span class="p">)</span><span class="w">

</span><span class="c1">## Loading required package: lattice
</span><span class="w">
</span><span class="c1">## Loading required package: ggplot2
</span><span class="w">
</span><span class="n">objControl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">trainControl</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'cv'</span><span class="p">,</span><span class="w"> </span><span class="n">number</span><span class="o">=</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">returnResamp</span><span class="o">=</span><span class="s1">'none'</span><span class="p">)</span><span class="w">

</span><span class="n">glm_caret_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="n">train_df</span><span class="p">[,</span><span class="n">setdiff</span><span class="p">(</span><span class="nf">names</span><span class="p">(</span><span class="n">train_df</span><span class="p">),</span><span class="n">outcome_name</span><span class="p">)],</span><span class="w"> 
                         </span><span class="n">as.factor</span><span class="p">(</span><span class="n">ifelse</span><span class="p">(</span><span class="n">train_df</span><span class="p">[,</span><span class="n">outcome_name</span><span class="p">]</span><span class="o">==</span><span class="m">1</span><span class="p">,</span><span class="s1">'yes'</span><span class="p">,</span><span class="s1">'no'</span><span class="p">)),</span><span class="w"> 
                         </span><span class="n">method</span><span class="o">=</span><span class="s1">'glm'</span><span class="p">,</span><span class="w"> 
                         </span><span class="n">trControl</span><span class="o">=</span><span class="n">objControl</span><span class="p">,</span><span class="w">
                         </span><span class="n">preProc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"center"</span><span class="p">,</span><span class="w"> </span><span class="s2">"scale"</span><span class="p">))</span><span class="w">

</span><span class="n">summary</span><span class="p">(</span><span class="n">glm_caret_model</span><span class="p">)</span><span class="w">

</span><span class="c1">## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2566  -0.6921  -0.3647   0.7062   2.3569  
## 
## Coefficients:
##                          Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)               -0.8444     0.1429  -5.911 3.41e-09 ***
## Number_times_pregnant      0.3025     0.1612   1.876  0.06067 .  
## Plasma_glucose             1.2242     0.1687   7.256 4.00e-13 ***
## Diastolic_blood_pressure  -0.3798     0.1452  -2.615  0.00893 ** 
## BMI                        0.7898     0.1647   4.795 1.62e-06 ***
## Age                        0.3843     0.1644   2.337  0.01941 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 480.61  on 366  degrees of freedom
## Residual deviance: 338.16  on 361  degrees of freedom
## AIC: 350.16
## 
## Number of Fisher Scoring iterations: 5
</span></code></pre>
</div>
<p><br /><br />
In the <code class="highlighter-rouge">Coefficients</code> section, we confirm that <code class="highlighter-rouge">Diastolic_blood_pressure</code> has a negative influence on the outcome, while <code class="highlighter-rouge">Plasma_glucose</code> has a positive influence.</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">test_df</span><span class="p">[</span><span class="m">359</span><span class="p">,]</span><span class="w">

</span><span class="c1">##     Number_times_pregnant Plasma_glucose Diastolic_blood_pressure  BMI Age
## 680                     2             56                       56 24.2  22
##     Class
## 680     0
</span><span class="w">
</span><span class="n">test_df</span><span class="p">[</span><span class="m">71</span><span class="p">,]</span><span class="w">

</span><span class="c1">##     Number_times_pregnant Plasma_glucose Diastolic_blood_pressure  BMI Age
## 154                     8            188                       78 47.9  43
##     Class
## 154     1
</span><span class="w">
</span></code></pre>
</div>
<p><br /><br />
<b>Once again, thanks Lucas for the actionable insights explorer!!</b></p>
		
</div>
    

		</div>		 
	 </div>   
 
</main>
{% include mid_point_ad.html %}

{% include footer.html %}
  </body>
</html>
